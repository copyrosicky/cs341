% vim: set expandtab tabstop=4 shiftwidth=4 textwidth=80:

\documentclass{article}
\usepackage[inner=2.5cm,outer=2.5cm,bottom=1.8cm,top=1.8cm]{geometry}
\usepackage{graphicx}% Include figure files
\graphicspath{ {./graphics/} }
%\usepackage{sidecap} % can be used for figures with side caption
\usepackage{caption}
\usepackage{numprint} % rounds numbers in tables
\npdecimalsign{.}
\nprounddigits{5}
\captionsetup{justification=centering}
\usepackage{wrapfig}
\usepackage{tabu} % for changing individual row fonts
%\usepackage{dcolumn} % Align table columns on decimal point
%\usepackage{bm} % bold math
\usepackage{amssymb, amsmath}
\usepackage[mathscr]{euscript}
\usepackage{dsfont}

\title{Improving Product Search with Session Re-Rank\\
    \large{a Walmart data mining project}}
\author{Charles Celerier (cceleri), Bill Chickering (bchick),
        and Jamie Irvine (jirvine)}

\begin{document}

\maketitle

Walmart.com maintains an online catalog of over 2M products. Consequently,
enabling users to quickly find products that conform to their specific needs and
tastes is especially challenging. Given the difficulty of its task,
Walmart.com's product search engine does an impressive job in interpretting the
user-provided query and rapidly returning relevant results. Yet, there remains
highly significant information that is not fully leveraged. The details of a user's online shopping session are indicative of a user's intent and
compliment---indeed, provide context for---the user-provided query. In this report
we describe and analyze a ranking scheme we call {\em Session Re-Rank} ({\em SRR}) that can
potentially induce a large increase in both click-through-rates and conversions
on the first page of query results.

\section{The Technique}\label{sec:technique}

{\em SRR} works by comparing previously clicked items with the top
$N$ items returned by the search engine in reponse to a query. Items to be shown
that are sufficiently similar to previously clicked items are promoted. The
extent (i.e. number of positions) of the promotion for a particular item is a
function of its similarity to previously clicked items, its original position,
and the promotions of other items.

The similarity between an item to be shown and a previously clicked item is
determined within five distinct vector spaces: {\em click-space}, {\em
cart-space}, {\em query-space}, {\em title-space}, {\em item-space}. The
non-unique representation of an item within each of these spaces may be thought
of as a binary vector or a set of objects. (MapReduce jobs process historic
query data to construct indexes whose keys are itemids and values are lists of
the appropriate objects. Great care went into ensuring that index entries can be
accessed in $\mathcal{O}(1)$ and that two entries can be merged to compute their
intersection or union in linear time.) The similarity $J_s(A, B)$ of two items,
$A$ and $B$, within a particular space $s$ is determined using Jaccard
similarity. Similarities within particular spaces are then weighted and summed
to determine the composite similarity
\begin{equation}\label{eqn:similarity_metric}
    S(A, B) = \sum_s{C_s(J_s(A, B))^{\alpha_s}},
\end{equation}
where $C_s$ and $\alpha_s$ are tuning parameters. The score $\sigma$ attributed
to an item to be shown is then the summation of composite similarities between
itself and all previously clicked items plus the click-through-rate (CTR)
$\Gamma_i$ of the item's original position $i$
\begin{equation*}
    \sigma = \sum_{B \in P}{S(A, B)} + \Gamma_i,
\end{equation*}

where $P$ is the set of previously clicked items. 

\section{Similarity Spaces}\label{sec:similarity_spaces}

The premise behind {\em click-space} is that two items are similar if they are
both clicked within the same online shopping session. The dimensions, or
objects, of this space are therefore past user-sessions. The {\em clicks-index}
for the data presented in this report was contructed using approximately half of
the Walmart provided data, or about 60M queries (about 120M page views).

{\em Cart-space} is based on the notion that two items are similar is they ever
appear in a shopping cart together. The objects of this space are therefore
shopping carts. The {\em clicks-index} for the data presented in this report was
contructed using approximately half of the Walmart provided data.

Items are also considered similar if they appear in a query together. The
objects of {\em query-space} are therefore queries. We make a distinction,
however, between {\em user-queries} and {\em unique-queries}. The former are the
well-defined entities within the raw Walmart data. The latter is an abstraction
based on the notion that multiple {\em user-queries} can correspond to a single
{\em unique-query}. To derive {\em unique-queries} from our data, we cluster
{\em user-queries} as follows: two {\em user-queries} with the same search
attributes (e.g. category or price filters) are considered the same {\em
unique-query} if the strings constructed by concatenating the space-seperated,
stemmed (we use the Python stemming.porter2 module), forced to lower-case terms
from each of their rawqueries are equal. We point out that while we achieved
better results with this policy compared to simply using {\em user-queries}, we
have no reason to believe that this is the ideal way to cluster queries for use
within {\em SRR}. Indeed, we believe one way to improve {\em SRR} is to optimize 
the query clustering policy.

{\em Title-space} is straightfoward: each item is associated with a set of terms
from its title. We ignore case, but at present do not stem, discard stop words,
or weight terms in any way. 

Finally, the structure of {\em item-space} is unique because it involves a level
of indirection. The premise here is that if items A and B are clicked in a
single user-session and items A and C are clicked in another user-session, that
items B and C are similar because they have item A in common. In this way, a
large number of relationships between items is created. {\em Item-space}
resembles {\em click-space} in that if two items are clicked during a single
session, they will have nonzero similarity. It differs from {\em click-space} in
two key respects, however. First, items that have historically never been
clicked in the same session can have nonzero similarity if they were each
clicked with a common third item. Second, if items are clicked together in many
sessions this will increase their Jaccard similarity in {\em click-space} but
not in {\em item-space}.

\section{An Example}


%\begin{wrapfigure}[4]{r}{0.1\textwidth}
%    \vspace{-10pt}
%    \centering
%    \includegraphics[width=0.1\textwidth]{bottled_water.eps}
%\end{wrapfigure}

%\begin{wrapfigure}[5]{l}{0.08\textwidth}
%    \vspace{-11pt}
%    \centering
%    \includegraphics[width=0.1\textwidth]{water_cooler.eps}
%\end{wrapfigure}

%\begin{wrapfigure}[5]{r}{0.1\textwidth}
%    \vspace{-10pt}
%    \centering
%    \includegraphics[width=0.1\textwidth]{arrowhead_water.eps}
%\end{wrapfigure}

To illustrate the efficacy of our technique, we present a real query example.
The only fictitious part of the example will be our shopper's name, David.
David is interested in the ``Primo Ceramic Crock Water Cooler with Stand'' and
clicks on this item during his session. Sometime later he navigates to the
``Grocery $\rightarrow $Beverages $\rightarrow $Water'' category and searches
for ``water''. He is presented with 300+ results and clicks 90th item, a 3 liter
jug of water.

The top six original results are compared to the {\em SRR} results in Table
\ref{tab:compare_orderings} where the first and third column represent the
original ranking presented to David. Note that we rerank the 90th item from the
original results to be the 3rd item in the {\em SRR} results.
\begin{table}[htbp!]
    \centering
    \begin{tabu}{| c | p{7cm} | c | p{7cm} |}
        \hline
\rowfont{\bfseries} \multicolumn{2}{|c|}{Original Ordering} & \multicolumn{2}{c|}{{\em SRR} Ordering} \\ \hline
        \hline
1  &   Great Value Purified Water, 24ct                 & 1  & Great Value Purified Water, 24ct \\ \hline
2  &   Nestle Waters Bottled Spring Water, 24ct         & 2  & Nestle Waters Bottled Spring Water, 24ct \\ \hline
3  &   Voss Water, 16.9 oz (Pack of 24)                 & 90 & Arrowhead Mountain Spring Water, 3 l \\ \hline
4  &   Clear American Cherry Sparkling Water, 1 l, 12pk & 63 & Great Value: Distilled Water, 1 Gal \\ \hline
5  &   Clear American Water, 1 l, 12ct                  & 38 & Arrowhead Mountain Spring Water, 2.5gal \\ \hline
6  &   Clear American Peach Sparkling Water, 1 l, 12ct  & 8  & Clear American Mandarin Orange Sparkling Water, 1 l, 12pk \\ \hline
    \end{tabu}
    \caption{Original Ordering vs. {\em SRR} Ordering for ``water'' query}
    \label{tab:compare_orderings}
\end{table}
Tables \ref{tab:original_sim_scores} and \ref{tab:rerank_sim_scores} show the
similarity scores from each index for each item in the two orderings compared to
David's lone previously clicked item. In each of these tables, the first column
correponds to Table \ref{tab:compare_orderings} and the second column is the
similarity metric from Equation (\ref{eqn:similarity_metric}) using our
optimized tuning parameters.
\begin{table}[htbp!]
    \centering
    \begin{tabu}{ c | n{1}{5} | n{1}{5} | n{1}{5} | n{1}{5} | n{1}{5} | n{1}{5} | n{1}{5} |}
        \cline{2-8}
        \rowfont{\bfseries} & \multicolumn{1}{c|}{S} & \multicolumn{1}{c|}{CTR} & \multicolumn{1}{c|}{Clicks} & \multicolumn{1}{c|}{Items} & \multicolumn{1}{c|}{Carts} & \multicolumn{1}{c|}{Queries} & \multicolumn{1}{c|}{Titles}  \\ \hline
        %% raw similarity scores
        %\multicolumn{1}{|c|}{1} & 0.943746587049 & 0.0754 & 0.00227790432802 &  0.017955801105   &  0.0 &  0.0104712041885   &  0.0909090909091 \\ \hline
        %\multicolumn{1}{|c|}{2} & 1.20210752946  & 0.0390 & 0.00389105058366 &  0.0251450676983  &  0.0 &  0.0254833040422   &  0.0833333333333 \\ \hline
        %\multicolumn{1}{|c|}{3} & 0.357056025563 & 0.0254 & 0.0              &  0.0227882037534  &  0.0 & 0.00555212831585   &  0.0769230769231 \\ \hline
        %\multicolumn{1}{|c|}{4} & 0.75268102742  & 0.0195 & 0.0016835016835  &  0.0233918128655  &  0.0 &  0.0016339869281   &  0.0666666666667 \\ \hline
        %\multicolumn{1}{|c|}{5} & 0.736731785955 & 0.0153 & 0.00173310225303 &  0.016077170418   &  0.0 &  0.00390015600624  &  0.0666666666667 \\ \hline
        %\multicolumn{1}{|c|}{6} & 0.174834647629 & 0.0129 & 0.0              &  0.00954198473282 &  0.0 &  0.000793021411578 &  0.0666666666667 \\ \hline
        
        % tuned scores
        \multicolumn{1}{|c|}{1} & 0.943746587049 & 0.0754 & 0.525001355894 & 0.160481073286 & 0.0 & 0.153493353029 & 0.02937080484    \\ \hline
        \multicolumn{1}{|c|}{2} & 1.20210752946 & 0.039 & 0.686161147707 & 0.210098155039 & 0.0 & 0.239452362893 & 0.0273958638253    \\ \hline
        \multicolumn{1}{|c|}{3} & 0.357056025563 & 0.0254 & 0.0 & 0.194190538676 & 0.0 & 0.11176890762 & 0.0256965792667              \\ \hline
        \multicolumn{1}{|c|}{4} & 0.75268102742 & 0.0195 & 0.451335466924 & 0.198294695203 & 0.0 & 0.060633906259 & 0.0229169590345   \\ \hline
        \multicolumn{1}{|c|}{5} & 0.736731785955 & 0.0153 & 0.457935991834 & 0.146901991555 & 0.0 & 0.0936768435316 & 0.0229169590345 \\ \hline
        \multicolumn{1}{|c|}{6} & 0.174834647629 & 0.0129 & 0.0 & 0.0967767348166 & 0.0 & 0.0422409537777 & 0.0229169590345           \\ \hline
    \end{tabu}
    \caption{Index similarity scores of the top six original results to \\ ``Primo Ceramic Crock Water Cooler with Stand''}
    \label{tab:original_sim_scores}
\end{table}
\begin{table}[htbp!]
    \centering
    \begin{tabu}{ c | n{1}{5} | n{1}{5} | n{1}{5} | n{1}{5} | n{1}{5} | n{1}{5} | n{1}{5} |}
        \cline{2-8}
        \rowfont{\bfseries} & \multicolumn{1}{c|}{S} & \multicolumn{1}{c|}{CTR} & \multicolumn{1}{c|}{Clicks} & \multicolumn{1}{c|}{Items} & \multicolumn{1}{c|}{Carts} & \multicolumn{1}{c|}{Queries} & \multicolumn{1}{c|}{Titles}  \\ \hline
        %% raw similarity scores
        %\multicolumn{1}{|c|}{1 } & 0.943746587049 & 0.0754     & 0.00227790432802 &  0.017955801105 &  0.0 &  0.0104712041885 &  0.0909090909091  \\ \hline
        %\multicolumn{1}{|c|}{2 } & 1.20210752946  & 0.0390     & 0.00389105058366 &  0.0251450676983 &  0.0 &  0.0254833040422 &  0.0833333333333 \\ \hline
        %\multicolumn{1}{|c|}{90} & 0.95310150677  & 0.00022623 & 0.00357142857143 &  0.027027027027 &  0.0 &  0.000920810313076 &  0.0833333333333\\ \hline
        %\multicolumn{1}{|c|}{63} & 0.949541641831 & 0.00049366 & 0.00261096605744 &  0.0338345864662 &  0.0 &  0.00385802469136 &  0.0833333333333\\ \hline
        %\multicolumn{1}{|c|}{38} & 0.93768536446  & 0.000914   & 0.00344234079174 &  0.0331384015595 &  0.0 &  0.0 &  0.0909090909091             \\ \hline
        %\multicolumn{1}{|c|}{8 } & 0.896706599209 & 0.01       & 0.00355239786856 &  0.0248565965583 &  0.0 &  0.0 &  0.0666666666667             \\ \hline

        % tuned scores
        \multicolumn{1}{|c|}{1} & 0.943746587049 & 0.0754 & 0.525001355894 & 0.160481073286 & 0.0 & 0.153493353029 & 0.02937080484        \\ \hline
        \multicolumn{1}{|c|}{2} & 1.20210752946 & 0.039 & 0.686161147707 & 0.210098155039 & 0.0 & 0.239452362893 & 0.0273958638253        \\ \hline
        \multicolumn{1}{|c|}{3} & 0.95310150677 & 0.00022623 & 0.657375735134 & 0.222586393133 & 0.0 & 0.0455172846776 & 0.0273958638253  \\ \hline
        \multicolumn{1}{|c|}{4} & 0.949541641831 & 0.00049366 & 0.562073743338 & 0.266408875605 & 0.0 & 0.0931694990625 & 0.0273958638253 \\ \hline
        \multicolumn{1}{|c|}{5} & 0.93768536446 & 0.000914 & 0.645386113734 & 0.262014445886 & 0.0 & 0.0 & 0.02937080484                  \\ \hline
        \multicolumn{1}{|c|}{6} & 0.896706599209 & 0.01 & 0.65562195059 & 0.208167689584 & 0.0 & 0.0 & 0.0229169590345                    \\ \hline
    \end{tabu}
    \caption{Index similarity scores of the top six {\em SRR} results to \\ ``Primo Ceramic Crock Water Cooler with Stand''}
    \label{tab:rerank_sim_scores}
\end{table} 

The similarity scores from {\em title-space} are easily calculated by hand. We
will show the calculation for the {\em item-space} similarity for the 90th item
in the original ranking. By referring to the corresponding index for {\em
item-space}, we can recall that the 90th item was clicked in a same session with
39 different items and the previously clicked item was clicked in a same session
with 455 different items. The titles of the 13 items found in common are:
\begin{itemize}
    \item[] Great Value: Distilled Water, 1 Gal
    \item[] Nestle Waters Bottled Spring Water, 24ct
    \item[] Primo Mineral Water, 5 gal
    \item[] Deer Park Sumo Bottle Natural Spring Water, 3l
    \item[] Arrowhead Mountain Spring Water, 3l
    \item[] PUR Advanced Faucet Water Filter Vertical - Chrome
    \item[] Ozarka Natural Spring Water
    \item[] Formula 409 All Purpose Lemon Scented Cleaner, 32 fl oz
    \item[] Great Value Spring Water, 1 gal
    \item[] Nestle Pure Life Purified Water, .5l, 35pk
    \item[] Arrowhead Mountain Spring Water, 2.5gal
    \item[] Primo Ceramic Crock Water Cooler with Stand
    \item[] Augason Farms Emergency Water Storage Kit
\end{itemize}
We then calculate the similarity of the 90th item and the previously clicked
item in {\em item-space} to be $13/(455+39-13)=0.0270$.

We believe this example illustrates how our technique can form subtle
relationships between items based on historical user session data. In this case,
we have a shopper who had an interest in a water cooler and subsequently made a
query for ``water''. If the shopper had been in a brick-and-mortar store, he
likely would have been directed to the section of the store selling water jugs
to use with the cooler he had picked up. In this case, {\em SRR} recognized
David's previous click on a water cooler, related that water cooler to large
water jugs, and showed David the water jug he was looking for all along.

\section{The Data}

Walmart.com has generously supplied us with a large dataset consisting of about
250M pageviews comprising about 120M query results which occurred over about 30
days. The data includes the user-provided rawqueries together with search
attributes, visitorIds and sessionIds, shown items, clicked items, which items
were placed in a shopping cart, and which items were ultimately purchased. In
addition, they have provided detailed item information including title,
description, category, and other details. The query data was randomized with
respect to search time and then segregated into three disjoint sets. The first
set, which consists of about half of the data, was re-structured into indexes
that form four of the similarity spaces ({\em click-space}, {\em cart-space}, 
{\em query-space}, and {\em item-space}) we use to identify relationships between 
items in realtime (the remaining similarity space, {\em title-space}, was compiled
separately using the provided item data). The second set, which consists of less 
than 5\% of the data, was used for testing and optimization allowing us to refine 
our technique and tune its parameters. And the third set, which includes about 
10\% of the data, was used in the experiments described and analyzed in this 
report.

\section{The Technique vs The Experiment}

An important distinction should be made between the {\em SRR}
technique and the experiment described in this report. Both the technique and
the experiment leverage the provided data---however, the experiment is
a simulation and a limited one at that. A key limitation is that the provided
query data is confined to what the user was actually shown. That is, the search
engine may have identified several pages worth of results in response to a {\em
user-query}, but our dataset consists only of those pages actually seen by the user.
Meanwhile, the concept behind the {\em SRR} technique calls for a
search engine to deliver to the algorithm the top $N$ items in response to a
{\em user-query} {\em independent of the number of items ultimately shown to the
user}. As a consequence, it is difficult, if not impossible, to simulate our
technique using shown query results that are truncated because a user only
viewed one or two pages.  Even more generally, the use of historic data to
demonstrate the consequences of a online ranking algorithm is intrinsically
limited by the fact that one cannot be certain how users would have behaved if
presented with different results. Nonetheless, we have done our best to conduct
the most fair and informative experiment and analysis.

\section{The Experiment}

The goal for the experiment is to simulate {\em SRR} using the
provided historical query data, which is limited to what users were actually
shown. Since an online implementation of our technique would receive the top $N$
items from the search engine for re-ranking prior to showing any results to a
user, the final ranking would be independent of the total number of items
actually shown (e.g. the number of page views requested by a user). Our test 
set $\chi$ therefore consists solely of queries where either all items in the
query resultset or at least $N=100$ were shown to the user. For example, if
the search engine found only 13 items in response to a query, all of these
items were shown to the user on a single page. In this case, we have the
complete query resultset and can therefore determine how {\em SRR}
would have reordered the shown items. At the same time, if more than $N=100$
were shown to the user, we can determine the reordering regardless of whether
the query resultset is truncated since {\em SRR} only considers
and re-ranks the first $N=100$ items.

To construct $\chi$ we therefore must discard all queries with a number of
shown results less than $N=100$ that are also divisible by 16. The reason for 
this is that Walmart.com provides two options for the number of items shown per
page: 16 or 32. Thus, by performing the experiment on this subset of the data we
precluded queries where the top $N$ items are not available to our algorithm.
The choice of $N=100$, meanwhile, is somewhat arbitrary and was made by
balancing our desire for a large test set with our desire to use a value
comparable to what would be appropriate for an online implementation. It is
therefore quite possible that a
larger value of $N$ (e.g. 1000) would achieve better results in the actual
online scenario.

While we must constrain $\chi$ in this way given the nature of the available 
data, we stress that this subset is certainly biased with respect to queries 
in general. For starters, queries with short resultsets are more likely to 
have all of their resultset seen by a user, and therefore, are more likely to 
be included in $\chi$. It is not clear, however, if this particular bias
tends to under- or overestimate the effectiveness of {\em SRR}
since, as we will show, it is more effective on longer query results. Similarly,
highly qualified queries---e.g. through the use of category or price
filters---tend to have shorter resultset, and hence, are more likely to be
included in $\chi$.

Just as interesting are the ways in which the queries and resultsets of $\chi$
are not biased. In Fig. \ref{fig:ctr_vs_position} we show click-through-rate (CTR)
as a function of position of the original data (i.e. not re-ranked) for both
$\chi$ and query results in general. The two curves shown in the figure are
quite similar indicating that the quantity and distribution of clicks within $\chi$ 
are essentially representative of those in general. A few other features of
this figure warrant brief comment. First, we see that $\chi$ has a larger CTR
for the top two positions. This is likely due to the fact that highly qualified
queries, which have shorter results and higher CTRs,  make up a higher proportion 
of $\chi$ than queries in general. While this difference does results in a slightly
higher frequency of clicks in $\chi$, we have no reason to believe that this alone
results in significant bias. Next, we see in the red curve a discontinuity appears at
position 17 as a result of the pagebreak. This discontinuity is absent from the
blue curve due to the omission of truncated results. In its place, we find a local 
maximum at position 17 that we ascribe to users' tendency to disproportionately
click on the topmost item shown on a page. Indeed, an additional, though smaller, 
maximum exists at position 33 corresponding to the topmost position of the third
page also exists (not shown).

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{CTRcompare.png}
    \caption{CTR as a function of position from the original data (i.e. not re-ranked) for the test set $\chi$ (blue) and all queries (red).}
    \label{fig:ctr_vs_position}
\end{figure}

A key feature of {\em SRR} is that it can only re-rank the results of a query if 
user has previously clicked on an item during their an online session.
Consequently, {\em SRR} can only affect the subset of queries that occur in a 
session with previous clicks. We denote this subset of queries as $\zeta$. It turns 
out that 25\% of all queries are in $\zeta$. Moreover, 28\% of all clicks and 
33\% of all purchases occur within the query resultsets of $\zeta$, since
there is a correlation between previous clicks and clicks/purchases in a query. 
Thus an impact to this subset can have a significant impact overall.

Within the dataset $\zeta$, we call the subset of queries restricted in this way 
$\chi$. $\chi$ makes up 25\% of $\zeta$, accounting for a total of 6\% of all queries.

\section{Metrics}

A true test of the effectiveness of {\em SRR} would require online A/B
testing. In the meantime, we can simulate the effect of {\em SSR} by running it
on historical data and examining the new positions of clicked and purchased items.
Assuming the user would have clicked or purchased the same items in this new ordering, 
we can compare the distribution of clicks in the original ranking to that of yielded 
by {\em SRR}.

To compare two rankings of shown items, we focus on three key metrics. Our primary 
metric is the first-page CTR $\mathscr{C}$, defined as the likelihood an item 
presented on the first page receives a click. As noted above, 75\% of all queries 
are only one page long. This shows the importance of bringing desirable items to the 
first page and it motivates our focus on $\mathscr{C}$. We calculate this metric by 
counting the number of items in first-page positions that were clicked and dividing 
by the number of total items in first-page positions. More formally
\begin{equation*}
    \mathscr{C} = \frac{\sum_{q \in Q}\sum_{i=1}^{L_q}\mathds{1}\left\{click\; @\; i \wedge i \leq 16\right\}}{\sum_{q \in Q}\sum_{i=1}^{L_q}\mathds{1}\left\{i \leq 16\right\}},
\end{equation*}

where $Q$ is a set of query results, $L_q$ is the number of results for query $q$, and
the number 16 is due to the fact that 16 items are shown on a page.

Our second metric is the purchasing rate of items on the first page $\mathscr{P}$. This is 
similar to $\mathscr{C}$, except here we consider purchases per first-page item 
instead of clicks. It is calculated as the number of items in first-page positions that 
were purchased divided by the total number of items in first-page positions. The 
importance of position for purchases is even stronger than that for clicks. While 70\% of 
all clicks were presented on the first page, that number is 85\% for purchases. Formally,
we have
\begin{equation*}
    \mathscr{P} = \frac{\sum_{q \in Q}\sum_{i=1}^{L_q}\mathds{1}\left\{purchase\; @\; i \wedge i \leq 16\right\}}{\sum_{q \in Q}\sum_{i=1}^{L_q}\mathds{1}\left\{i \leq 16\right\}}.
\end{equation*}

The first two metrics focus on whether or not a desirable item was presented on the 
first page. To obtain a more granular picture of where desirable items are positioned, 
we also compute a third metric which we call {\em click-position score} $\mathscr{S}$. 
Somewhat similar to normalized discounted cumulative gain (NDCG), which is a common metric
of search engine results, this score weighs the value of a clicked item by its position, 
giving higher weights to items closer to the top. For $\mathscr{S}$, the weight given to 
a click in position $i$ is the CTR at position i $\Gamma_i$. In this way, we equate how 
often users click on a certain position to with how valuable it is to put a desirable 
item there. Formally, we define {\em click-position score} as
\begin{equation*}
    \mathscr{S} = \frac{1}{\left\vert{Q}\right\vert}\sum_{q \in Q}\sum_{i=1}^{L_q}\mathds{1}\left\{click\; @\; i\right\}\Gamma_i.
\end{equation*}

These metrics give a sense of how successful a ranking scheme is. Each one looks at a slightly 
different aspect of the ordering. Indeed, optimizing for one metric does not necessarily 
optimize for the others. We focus our optimizations and primary analysis on $\mathscr{C}$
as we believe it is the simplest and has the clearest impact to overall CTRs.

\section{Results}

Figure \ref{fig:avg_clicks_position_score} shows the clicks-position score $\mathscr{S}$ 
as a function of each of the coefficients $C_s$ discussed in sections \ref{sec:technique} 
and \ref{sec:similarity_spaces}. Here, we vary a single coefficient, corresponding to
to one of the five similarity spaces---{\em click-space}, {\em cart-space}, {\em 
query-space}, {\em title-space}, or {\em item-space}---while setting the others to zero.
In each case, as $C_s$ is increased from zero, the degree of reordering is enhanced.
And for each coefficient, this reordering is accompanied by an increase in $\mathscr{S}$
indicating a greater concentration of clicked items, on average, near the top positions 
of query results. Moreover, with the exception of {\em title-space}, the score increases 
monotonically with each coefficient.



\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{000050_0_48chunk_k100_i2_n100_avg_click_position_score_0-0_16.png}
    \caption{average clicks position score}
    \label{fig:avg_clicks_position_score}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{moneyshot.png}
    \caption{Comparison of the original ranking (blue) to that of randomly re-ranking (grey) and {\em SRR} (blue) using all three metrics: front-page CTR $\mathscr{C}$, front-page purchase rate $\mathscr{P}$, and click-position score $\mathscr{S}$.}
    \label{fig:}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{scorebylen.png}
    \caption{Change in click-position score $\mathscr{S}$ as a function of query length $L_q$.}
    \label{fig:}
\end{figure}

\end{document}

