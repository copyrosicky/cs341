% vim: set expandtab tabstop=4 shiftwidth=4 textwidth=80:

\documentclass{article}
\usepackage[inner=2.5cm,outer=2.5cm,bottom=1.8cm,top=1.8cm]{geometry}
\usepackage{graphicx}% Include figure files
\graphicspath{ {./graphics/} }
%\usepackage{dcolumn} % Align table columns on decimal point
%\usepackage{bm} % bold math
\usepackage{amssymb, amsmath}

\title{Improving Product Search with Session Re-Rank\\
    \large{a Walmart data mining project}}
\author{Charles Celerier (cceleri), Bill Chickering (bchick),
        and Jamie Irvine (jirvine)}


\begin{document}

\maketitle

Walmart.com maintains an online catalog of over 2M products. Consequently,
enabling users to quickly find products that conform to their specific needs and
tastes is especially challenging. Given the difficulty of its task,
Walmart.com's product search engine does an impressive job in interpretting the
user-provided query and rapidly returning relevant results. Yet, there remains
highly significant information that is not fully leveraged. The details of a
user's online shopping session are indicative of a user's intent and
compliment--indeed, provide context for--the user-provided query. In this report
we describe and analyze a ranking scheme we call {\em Session Re-Rank} that can
potentially induce a large increase in both click-through-rates and conversions
on the first page of query results.

\section{The Technique}

{\em Session Re-Rank} works by comparing previously clicked items with the top
$N$ items returned by the search engine in reponse to a query. Items to be shown
that are sufficiently similar to previously clicked items are promoted. The
extent (i.e. number of positions) of the promotion for a particular item is a
function of its similarity to previously clicked items, its original position,
and the promotions of other items.

The similarity between an item to be shown and a previously clicked item is
determined within five distinct vector spaces: {\em click-space}, {\em
cart-space}, {\em query-space}, {\em title-space}, {\em item-space}. The
non-unique representation of an item within each of these spaces may be thought
of as a binary vector or a set of objects. (MapReduce jobs process historic
query data to construct indexes whose keys are itemids and values are lists of
the appropriate objects. Great care went into ensuring that index entries can be
accessed in $\mathcal{O}(1)$ and that two entries can be merged to compute their
intersection or union in linear time.) The similarity $J_s(A, B)$ of two items,
$A$ and $B$, within a particular space $s$ is determined using Jaccard
similarity. Similarities within particular spaces are then weighted and summed
to determine the composite similarity
\begin{equation*}
    S(A, B) = \sum_s{C_s(J_s(A, B))^{\alpha_s}},
\end{equation*}
where $C_s$ and $\alpha_s$ are tuning parameters. The score $\sigma$ attributed
to an item to be shown is then the summation of composite similarities between
itself and all previously clicked items plus the click-through-rate (CTR)
$\Gamma_i$ of the item's original position $i$
\begin{equation*}
    \sigma = \sum_{B \in P}{S(A, B)} + \Gamma_i,
\end{equation*}

where $P$ is the set of previously clicked items. 

\section{Similarity Spaces}

The premise behind {\em click-space} is that two items are similar if they are
both clicked within the same online shopping session. The dimensions, or
objects, of this space are therefore past user-sessions. The {\em clicks-index}
for the data presented in this report was contructed using approximately half of
the Walmart provided data, or about 60M queries (about 120M page views).

{\em Cart-space} is based on the notion that two items are similar is they ever
appear in a shopping cart together. The objects of this space are therefore
shopping carts. The {\em clicks-index} for the data presented in this report was
contructed using approximately half of the Walmart provided data.

Items are also considered similar if they appear in a query together. The
objects of {\em query-space} are therefore queries. We make a distinction,
however, between {\em user-queries} and {\em unique-queries}. The former are the
well-defined entities within the raw Walmart data. The latter is an abstraction
based on the notion that multiple {\em user-queries} can correspond to a single
{\em unique-query}. To derive {\em unique-queries} from our data, we cluster
{\em user-queries} as follows: two {\em user-queries} with the same search
attributes (e.g. category or price filters) are considered the same {\em
unique-query} if the strings constructed by concatenating the space-seperated,
stemmed (we use the Python stemming.porter2 module), forced to lower-case terms
from each of their rawqueries are equal. We point out that while we achieved
better results with this policy compared to simply using {\em user-queries}, we
have no reason to believe that this is the ideal way to cluster queries for use
within {\em Session Re-Rank}. Indeed, we believe one way to improve {\em Session
Re-Rank} is to optimize the query clustering policy.

{\em Title-space} is straightfoward: each item is associated with a set of terms
from its title. We ignore case, but at present do not stem, discard stop words,
or weight terms in any way. 

Finally, the structure of {\em item-space} is unique because it involves a level
of indirection. The premise here is that if items A and B are clicked in a
single user-session and items A and C are clicked in another user-session, that
items B and C are similar because they have item A in common. In this way, a
large number of relationships between items is created. {\em Item-space}
resembles {\em click-space} in that if two items are clicked during a single
session, they will have nonzero similarity. It differs from {\em click-space} in
two key respects, however. First, items that have historically never been
clicked in the same session can have nonzero similarity if they were each
clicked with a common third item. Second, if items are clicked together in many
sessions this will increase their Jaccard similarity in {\em click-space} but
not in {\em item-space}.

\section{The Data}

Walmart.com has generously supplied us with a large dataset consisting of about
250M pageviews comprising about 120M query results which occurred over about 30
days. The data includes the user-provided rawqueries together with search
attributes, visitorids and sessionids, shown items, clicked items, which items
were placed in a shopping cart, and which items were ultimately purchased. In
addition, they have provided detailed item information including item title,
description, category, and other details. We leverage this data in three ways.
First, we re-structure the data into indexes that allow us to identify
relationships between items in realtime. Second, we isolate subsets of the data
in order to conduct experiments and perform measurements, which enable us to
refine our technique and optimize, or tune, the associated parameters. And
third, once developed and optimized, we conduct experiments on previously
withheld data and analyze the results in an effort to determine the efficacy, or
lack thereof, of our technique. 

\section{Our Technique vs Our Experiments}

An important distinction should be made between the {\em Session Re-Rank}
technique and the experiments described in this report. Both the technique and
the experiments leverage the provided data--however, the experiments are
simulations and limited ones at that. A key limitation is that the provided
query data is confined to what the user was actually shown. That is, the search
engine may have identified several pages worth of results in response to a {\em
user-query}, but the dataset consists only of those pages rendered to the user.
Meanwhile, the concept behind the {\em Session Re-Rank} technique calls for a
search engine to deliver to the algorithm the top $N$ items in response to a
{\em user-query} {\em independent of the number of items ultimately shown to the
user}. As a consequence, it is difficult to simulate our technique using shown
query results that are truncated because a user only viewed one or two pages. 
More generally, the use of historic data to demonstrate the consequences of a 
online ranking algorithm is intrinsically limited since one cannot be certain 
how users would have behaved if presented with different results. Nonetheless, 
we have done our best to conduct the most fair and informative experiments and 
analyses.

\section{Experiment 1}

The goal for experiment 1 is to simulate {\em Session Re-Rank} using the
provided historical query data, which is limited to what users were actually
shown. Since an online implementation of our technique would receive the top $N$
items from the search engine for re-ranking prior to showing any results to a
user, the final ranking would be independent of the total number of items
actually shown (e.g. the number of page views requested by a user). We therefore
limit the test set of experiment 1 to queries where either at least $N=100$ were
shown or the number shown is not divisible by 16. The reason for this is that
Walmart.com provides two options for the number of items shown per page: 16 or
32. By performing the experiment on this subset of the data we preclude queries
where the top $N$ items are not available to our algorithm. The choice of
$N=100$, meanwhile, is somewhat arbitrary and was made by balancing our desire
for a large test set with our desire to use a value comparable to what would be
appropriate for an online implementation. It is therefore quite possible that a
larger value of $N$ (e.g. 1000) would achieve better results in the actual
online scenario.

Confining the experiment to this subset of queries allows our algorithm to
re-rank the same number of 

\section{Results}

\begin{center}
    \begin{tabular}{| l | c |}
        \hline
        Dataset & 000050\_0.test\_data.filtered \\ \hline
        Index & 48chunk \\ \hline
        \hline
        \multicolumn{2}{|c|}{Parameters} \\ \hline
        coeff\_carts & free or 0.00 \\ \hline
        coeff\_clicks & free or 0.00 \\ \hline
        coeff\_ctr & 1.000 \\ \hline
        coeff\_item\_title & free or 0.00 \\ \hline
        coeff\_items & free or 0.00 \\ \hline
        coeff\_queries & free or 0.00 \\ \hline
        coeff\_rank & 0.000 \\ \hline
        ctr\_by\_position & 16chunk.CTRs.json \\ \hline
        exp\_carts & 0.400 \\ \hline
        exp\_clicks & 0.500 \\ \hline
        exp\_item\_title & 0.800 \\ \hline
        exp\_items & 0.800 \\ \hline
        exp\_queries & 0.500 \\ \hline
        exp\_rank & 1.300 \\ \hline
        insert\_position & 2 \\ \hline
        k & 100 \\ \hline
        n & 100 \\ \hline
  \end{tabular}
\end{center}

\begin{figure}[htbp!]
\centering
\includegraphics[width=\textwidth]{000050_0.48chunk.k100.i2.n100.percent_increase_position_score.0-0.25.eps}
\caption{\% increase of position score}
    \label{fig:percent_increase_of_position_score}
\end{figure}

\begin{figure}[htbp!]
\centering
\includegraphics[width=\textwidth]{000050_0.48chunk.k100.i2.n100.total_purchased_front_page.0-0.25.eps}
\caption{total purchases on the front page}
\label{fig:total_purchases_on_the_front_page}
\end{figure}

\section{An Example}

To illustrate the efficacy of our technique, we present a real query example.
The only fictitious part of the example will be our shopper's name David.

David narrows his search space by selecting the category
``Grocery$\rightarrow$Beverages$\rightarrow$Water'' and made a query for
``water''. For the first few pages, Walmart presents David with a large
variety of different brands and sizes of packs of bottled water. These appear to
be reasonable results for any shopper making such a general query.
Unfortunately, packs of bottled water were not what David wanted to find. David
ended up clicking on and the 90th item presented by Walmart, ``Arrowhead
Mountain Spring Water, 3 l''. %picture?

It turns out that before making his query for ``water'', David had clicked on
the item ``Primo Ceramic Crock Water Cooler with Stand''. %picture?
Our technique recognized this previous click as a signal that David would be
interested in water products that are similar to his interest in the water
cooler stand. The reordered results generated by our technique 

was looking for 

\end{document}
